{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Busdyx8hpS2O"
   },
   "source": [
    "# Install Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_9oG_OYEj1o"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mob5EOipzACl",
    "outputId": "8e16ec77-7a72-498b-abf0-c58265a7600c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n",
      "Not running in Colab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    thesis_repo='http://colab:colab@git.arpogaus.de/master-thesis/experiments.git'\n",
    "    %cd \"/content/drive/My Drive/colab-projects/master-thesis\"\n",
    "    if os.path.exists('./exp'):\n",
    "      %cd exp\n",
    "      !git fetch -p --force\n",
    "      !git checkout FETCH_HEAD\n",
    "    else:\n",
    "      !git clone $thesis_repo exp\n",
    "      %cd ./exp \n",
    "    !ls -la\n",
    "\n",
    "    !pip install -Ue .\n",
    "\n",
    "    !pip install -U git+https://github.com/MArpogaus/tensorflow-experiments@dev\n",
    "except:\n",
    "    %cd ..\n",
    "    print('Not running in Colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzjpWZkvpF1W"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "4bqu77YM1sIG",
    "outputId": "3c0188fe-6721-4a51-d22a-8547c53fc0ff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', context='paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import ConnectionPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import bijectors as tfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKHflCVeymVb"
   },
   "outputs": [],
   "source": [
    "import tfexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrQzSG0v3zM3"
   },
   "outputs": [],
   "source": [
    "from bernstein_paper.util.visualization import plot_patches\n",
    "from bernstein_paper.util.visualization import plot_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernstein_paper.distributions import MultivariateBernsteinFlow\n",
    "from bernstein_paper.distributions import MixedNormal\n",
    "from bernstein_paper.distributions import NormalDistribution\n",
    "from bernstein_paper.losses import PinballLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path='./configs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_list(path, ending=''):\n",
    "    if os.path.exists(path):\n",
    "        return [\n",
    "            os.path.join(path, file)\n",
    "            for file in os.listdir(path)\n",
    "            if file.endswith(ending)\n",
    "            and os.path.getsize(os.path.join(path, file)) > 0]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_model_stats(probabilistic_model, pvector):\n",
    "    if len(pvector) != 1:\n",
    "        dfs=[]\n",
    "        for i in range(len(pvector)):\n",
    "            dfs.append(probabilistic_model_stats(probabilistic_model, pvector[i][None,...]))\n",
    "        return pd.concat(dfs,ignore_index=True)\n",
    "    else:\n",
    "        dist = probabilistic_model(pvector)\n",
    "        if probabilistic_model == MultivariateBernsteinFlow:\n",
    "            flows = dist.distributions.model\n",
    "            mus = []\n",
    "            plus_sds = []\n",
    "            minus_sds = []\n",
    "            q05s = []\n",
    "            q1s = []\n",
    "            q2s = []\n",
    "            q8s = []\n",
    "            q9s = []\n",
    "            q95s = []\n",
    "            for flow in flows:\n",
    "                base_dist = flow.distribution\n",
    "                bijector = flow.bijector\n",
    "\n",
    "                mu = bijector.forward(base_dist.mean())\n",
    "                plus_sd = bijector.forward(base_dist.mean() + base_dist.variance())\n",
    "                minus_sd = bijector.forward(base_dist.mean() - base_dist.variance())\n",
    "                q05 = bijector.forward(base_dist.quantile(.05))\n",
    "                q1 = bijector.forward(base_dist.quantile(.1))\n",
    "                q2 = bijector.forward(base_dist.quantile(.2))\n",
    "                q8 = bijector.forward(base_dist.quantile(.8))\n",
    "                q9 = bijector.forward(base_dist.quantile(.9))\n",
    "                q95 = bijector.forward(base_dist.quantile(.95))\n",
    "\n",
    "                mus.append(np.squeeze(mu))\n",
    "                plus_sds.append(np.squeeze(plus_sd))\n",
    "                minus_sds.append(np.squeeze(minus_sd))\n",
    "\n",
    "                q05s.append(np.squeeze(q05))\n",
    "                q1s.append(np.squeeze(q1))\n",
    "                q2s.append(np.squeeze(q2))\n",
    "                q8s.append(np.squeeze(q8))\n",
    "                q9s.append(np.squeeze(q9))\n",
    "                q95s.append(np.squeeze(q95))\n",
    "\n",
    "            mu = np.array(mus).flatten()\n",
    "            sdp = np.array(plus_sds).flatten()\n",
    "            sdm = np.array(minus_sds).flatten()\n",
    "            q05 = np.array(q05s).flatten()\n",
    "            q1 = np.array(q1s).flatten()\n",
    "            q2 = np.array(q2s).flatten()\n",
    "            q8 = np.array(q8s).flatten()\n",
    "            q9 = np.array(q9s).flatten()\n",
    "            q95 = np.array(q95s).flatten()\n",
    "\n",
    "        elif isinstance(probabilistic_model, MixedNormal):\n",
    "            mu = np.squeeze(dist.mean())\n",
    "            sdp = mu + np.squeeze(dist.distributions.variance())\n",
    "            sdm = mu + -np.squeeze(dist.distributions.variance())\n",
    "            #q1 = dist.quantile(.1) # not implemented\n",
    "            #q2 = dist.quantile(.2) # not implemented\n",
    "            #q8 = dist.quantile(.8) # not implemented\n",
    "            #q9 = dist.quantile(.9) # not implemented\n",
    "            q05 = None\n",
    "            q1 = None\n",
    "            q2 = None\n",
    "            q8 = None\n",
    "            q9 = None\n",
    "            q95 = None\n",
    "        \n",
    "        elif probabilistic_model == PinballLoss.constrain_quantiles:\n",
    "            mu = None\n",
    "            sdp = None\n",
    "            sdm = None\n",
    "            q05 = dist[:,:,5].numpy().flatten()\n",
    "            q1 = dist[:,:,10].numpy().flatten()\n",
    "            q2 = dist[:,:,20].numpy().flatten()\n",
    "            q8 = dist[:,:,80].numpy().flatten()\n",
    "            q9 = dist[:,:,90].numpy().flatten()\n",
    "            q95 = dist[:,:,95].numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            mu = np.squeeze(dist.mean())\n",
    "            sdp = mu + np.squeeze(dist.variance())\n",
    "            sdm = mu - np.squeeze(dist.variance())\n",
    "            q05 = np.squeeze(dist.quantile(.05))\n",
    "            q1 = np.squeeze(dist.quantile(.1))\n",
    "            q2 = np.squeeze(dist.quantile(.2))\n",
    "            q8 = np.squeeze(dist.quantile(.8))\n",
    "            q9 = np.squeeze(dist.quantile(.9))\n",
    "            q95 = np.squeeze(dist.quantile(.95))\n",
    "\n",
    "        df = pd.DataFrame(dict(\n",
    "            mu=mu,\n",
    "            sdp=sdp,\n",
    "            sdm=sdm,\n",
    "            q05=q05,\n",
    "            q1=q1,\n",
    "            q2=q2,\n",
    "            q8=q8,\n",
    "            q9=q9,\n",
    "            q95=q95\n",
    "        ))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilistic_forecast(\n",
    "    parameter_model,\n",
    "    probabilistic_model,\n",
    "    x,\n",
    "    ax=plt):\n",
    "\n",
    "    pvector = parameter_model(x)\n",
    "    dist = probabilistic_model(pvector)\n",
    "    df = probabilistic_model_stats(probabilistic_model, pvector)\n",
    "\n",
    "    t=np.arange(len(df.mu))\n",
    "        \n",
    "    ax.plot(\n",
    "        t,\n",
    "        df.mu,\n",
    "        label='$\\mu$',\n",
    "        c=\"black\"\n",
    "    )\n",
    "    \n",
    "    ax.plot(t, df.sdp,\n",
    "               label='$\\mu + \\sigma$',\n",
    "               c=\"gray\")\n",
    "\n",
    "    ax.plot(t, df.sdm,\n",
    "               label='$\\mu - \\sigma$',\n",
    "               c=\"gray\")\n",
    "    \n",
    "    if df.sdp.isnull().sum() == 0:\n",
    "        ax.fill_between(\n",
    "            t,\n",
    "            df.sdp,\n",
    "            df.sdm,\n",
    "            alpha=0.2,\n",
    "            label='$Q(10%)$',\n",
    "            fc='gray'\n",
    "        )\n",
    "\n",
    "    \n",
    "    if not isinstance(probabilistic_model,MixedNormal):\n",
    "        ax.plot(t, df.q95,\n",
    "                   label='$q(95%)$',\n",
    "                   c=\"skyblue\")\n",
    "        ax.plot(t, df.q8,\n",
    "                   label='$q(80%)$',\n",
    "                   c=\"skyblue\")\n",
    "        ax.plot(t, df.q2,\n",
    "                   label='$q(20%)$',\n",
    "                   c=\"skyblue\")\n",
    "        ax.plot(t, df.q05,\n",
    "                   label='$q(5%)$',\n",
    "                   c=\"skyblue\")\n",
    "\n",
    "        ax.fill_between(\n",
    "            t,\n",
    "            df.q95,\n",
    "            df.q05,\n",
    "            alpha=0.2,\n",
    "            label='$Q(90%)$',\n",
    "            fc='skyblue'\n",
    "        )\n",
    "\n",
    "        ax.fill_between(\n",
    "            t,\n",
    "            df.q8,\n",
    "            df.q2,\n",
    "            alpha=0.5,\n",
    "            label='$Q(60%)$',\n",
    "            fc='skyblue'\n",
    "        )\n",
    "\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patch_ts(\n",
    "    x,\n",
    "    y,\n",
    "    historic_columns,\n",
    "    horizon_columns,\n",
    "    prediction_columns,\n",
    "    fig_kw={},\n",
    "    title=None):\n",
    "\n",
    "    columns = sorted(\n",
    "        set(historic_columns + horizon_columns + prediction_columns))\n",
    "    x_columns = sorted(set(historic_columns + horizon_columns))\n",
    "    # y_columns = sorted(prediction_columns)\n",
    "\n",
    "    x_column_ch = {k: c for c, k in enumerate(x_columns)}\n",
    "    # y_column_ch = {k: c for c, k in enumerate(y_columns)}\n",
    "\n",
    "    if len(y.shape) == 3:\n",
    "        figs = []\n",
    "        for b in range(y.shape[0]):\n",
    "            fig = plot_patch_ts(\n",
    "                x[b],\n",
    "                y[b],\n",
    "                historic_columns,\n",
    "                horizon_columns,\n",
    "                prediction_columns,\n",
    "                fig_kw=fig_kw,\n",
    "                title=title)\n",
    "            figs.append(fig)\n",
    "        return figs\n",
    "    else:\n",
    "        horizon_size = y.shape[0]\n",
    "        history_size = x.shape[0] - horizon_size\n",
    "\n",
    "        fig, ax = plt.subplots(2, **fig_kw)\n",
    "\n",
    "        if title is not None:\n",
    "            fig.suptitle(title, fontsize=24)\n",
    "\n",
    "        t = np.array(range(history_size + horizon_size))\n",
    "        t_hori = np.array(range(horizon_size)) + history_size\n",
    "\n",
    "        for k in columns:\n",
    "            if k in historic_columns and k in horizon_columns:\n",
    "                hist = x[:history_size, x_column_ch[k]].flatten()\n",
    "                hori = x[history_size:, x_column_ch[k]].flatten()\n",
    "                dat = np.concatenate([hist, hori]).flatten()\n",
    "                ax_idx = 0\n",
    "            elif k in historic_columns and k in prediction_columns:\n",
    "                hist = x[:history_size, x_column_ch[k]].flatten()\n",
    "                hori = y.flatten()\n",
    "                dat = np.concatenate([hist, hori]).flatten()\n",
    "                ax_idx = 1\n",
    "            ax[ax_idx].plot(t, dat, label=k.replace('_',' '))\n",
    "\n",
    "        ax[0].legend(loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flow(flow, y, ax=plt, color='skyblue'):\n",
    "    base_dist = flow.distribution\n",
    "    bijector = flow.bijector\n",
    "\n",
    "    dense_y = flow.prob(y).numpy()\n",
    "\n",
    "    #mu = np.squeeze(bijector.forward(base_dist.mean()))\n",
    "    #plus_sd = np.squeeze(bijector.forward(base_dist.mean() + base_dist.variance()))\n",
    "    #minus_sd = np.squeeze(bijector.forward(base_dist.mean() - base_dist.variance()))\n",
    "    \n",
    "    #ax.plot(\n",
    "    #    [mu,mu],\n",
    "    #    [np.min(dense_y),\n",
    "    #     flow.prob(mu.reshape(-1,1)).numpy()],\n",
    "    #    color='black',\n",
    "    #    lw=2\n",
    "    #)\n",
    "    #ax.plot(\n",
    "    #    [plus_sd,plus_sd],\n",
    "    #    [np.min(dense_y),flow.prob(plus_sd.reshape(-1,1)).numpy()],\n",
    "    #    '--',\n",
    "    #    color='green'\n",
    "    #)\n",
    "    #ax.plot(\n",
    "    #    [minus_sd,minus_sd],\n",
    "    #    [np.min(dense_y),flow.prob(minus_sd.reshape(-1,1)).numpy()],\n",
    "    #    '--',\n",
    "    #    color='green'\n",
    "    #)\n",
    "\n",
    "    def quant(p):\n",
    "        q = bijector.forward(base_dist.quantile(p))\n",
    "        return np.squeeze(q)\n",
    "\n",
    "    qs = [.05,.1,.2,.3,.4]\n",
    "    ax.fill_between(\n",
    "        np.squeeze(y),\n",
    "        np.squeeze(dense_y),\n",
    "        np.min(dense_y),\n",
    "        fc=color,\n",
    "        alpha=max(qs)\n",
    "    )\n",
    "    for i,q in enumerate(sorted(qs)):\n",
    "        ax.fill_between(\n",
    "            np.squeeze(y),\n",
    "            np.squeeze(dense_y),\n",
    "            np.min(dense_y),\n",
    "            where=(\n",
    "                (np.squeeze(y) > quant(q)) & (np.squeeze(y) < quant(1-q))\n",
    "            ),\n",
    "            fc=color,\n",
    "            alpha=q / max(qs)\n",
    "        )\n",
    "\n",
    "    ax.plot(\n",
    "        y,\n",
    "        dense_y,\n",
    "        '-',\n",
    "        color='white',\n",
    "        linewidth=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(dist, y, ax=plt, color='skyblue'):\n",
    "\n",
    "    dense_y = dist.prob(y).numpy()\n",
    "    \n",
    "    #mu = np.squeeze(dist.mean())\n",
    "    #plus_sd = mu + np.squeeze(dist.variance())\n",
    "    #minus_sd = mu - np.squeeze(dist.variance())\n",
    "\n",
    "    #ax.plot(\n",
    "    #    [mu,mu],\n",
    "    #    [np.min(dense_y),\n",
    "    #     dist.prob(mu.reshape(-1,1))],\n",
    "    #    color='black',\n",
    "    #    lw=2\n",
    "    #)\n",
    "    #ax.plot(\n",
    "    #    [plus_sd,plus_sd],\n",
    "    #    [np.min(dense_y),dist.prob(plus_sd.reshape(-1,1))],\n",
    "    #    '--',\n",
    "    #    color='green'\n",
    "    #)\n",
    "    #ax.plot(\n",
    "    #    [minus_sd,minus_sd],\n",
    "    #    [np.min(dense_y),dist.prob(minus_sd.reshape(-1,1))],\n",
    "    #    '--',\n",
    "    #    color='green'\n",
    "    #)\n",
    "\n",
    "    ax.fill_between(\n",
    "        np.squeeze(y),\n",
    "        np.squeeze(dense_y),\n",
    "        np.min(dense_y),\n",
    "        fc=color,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        y,\n",
    "        dense_y,\n",
    "        '-',\n",
    "        color='white',\n",
    "        linewidth=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def stacked_dist_plot(\n",
    "        parameter_model,\n",
    "        probabilistic_model,\n",
    "        x,\n",
    "        y,\n",
    "        hspace=-.85,\n",
    "        title=None\n",
    "    ):\n",
    "    yy = np.linspace(0-.5*np.abs(y.min()), y.max(), 200).astype(np.float32)\n",
    "    yy = yy[...,tf.newaxis]\n",
    "    horizon_size = len(y)\n",
    "\n",
    "    if isinstance(probabilistic_model,(BersteinFlow, MixedNormal, MixedLogNormal)):\n",
    "        dists = [\n",
    "            d\n",
    "            for d in probabilistic_model(parameter_model(x[None,...])).distributions.model\n",
    "        ]\n",
    "    else:\n",
    "        pv = parameter_model(x[None,...])\n",
    "        dists = [\n",
    "            tfd.Normal(loc=pv[:,d,0],scale=np.exp(pv[:,d,1])) for d in range(horizon_size)\n",
    "        ]\n",
    "    \n",
    "    horizon_size = len(dists)\n",
    "    \n",
    "    pal1 = sns.cubehelix_palette(horizon_size/2, rot=.55, light=.8, reverse=True).as_hex()\n",
    "    pal2 = sns.cubehelix_palette(horizon_size/2, rot=.55, light=.8).as_hex()\n",
    "    pal = pal1 + pal2\n",
    "\n",
    "    horizon_size = y.shape[0]\n",
    "    fig , ax = plt.subplots(horizon_size,figsize=(16,horizon_size*0.4), sharex=True, sharey=True)\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=24)\n",
    "\n",
    "    yaxis = ConnectionPatch(\n",
    "        xyA=(0,0),\n",
    "        xyB=(0,10),\n",
    "        coordsA='data',\n",
    "        coordsB='data',\n",
    "        axesA=ax[-1],\n",
    "        axesB=ax[0],\n",
    "        color='black',\n",
    "        lw=1.5,\n",
    "        arrowstyle='->'\n",
    "        #arrowstyle='Fancy, head_length=0.4, head_width=0.1, tail_width=0.3'\n",
    "    )\n",
    "    ax[0].add_artist(yaxis)\n",
    "        \n",
    "    for i in range(horizon_size):\n",
    "        if i >= 1:\n",
    "            con = ConnectionPatch(\n",
    "                xyA=(y[i-1],0),\n",
    "                xyB=(y[i],0),\n",
    "                coordsA='data',\n",
    "                coordsB='data',\n",
    "                axesA=ax[i-1],\n",
    "                axesB=ax[i],\n",
    "                color=pal[i],\n",
    "                lw=2+1*i/horizon_size,\n",
    "                alpha=0.8\n",
    "            )\n",
    "            ax[i].add_artist(con)\n",
    "        if isinstance(probabilistic_model,BersteinFlow):\n",
    "            plot_flow(dists[i],yy,ax=ax[i],color=pal[i])\n",
    "        else:\n",
    "            plot_dist(dists[i],yy,ax=ax[i],color=pal[i])\n",
    "        ax[i].plot(\n",
    "            y[i],\n",
    "            0,\n",
    "            '.',\n",
    "            color=pal[i],\n",
    "            markersize=8+8*i/horizon_size)\n",
    "\n",
    "        ax[i].set_facecolor((0,0,0,0))\n",
    "        ax[i].set_frame_on(False)\n",
    "        ax[i].set_title('')\n",
    "        ax[i].axis('off')\n",
    "\n",
    "        ax[i].text(0, 0.1, i, fontweight=\"bold\", fontsize='xx-large', color=pal[i],\n",
    "                   ha=\"left\", va=\"center\", transform=ax[i].transAxes)\n",
    "\n",
    "    fig.subplots_adjust(hspace=hspace)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chained_bijectors(flow):\n",
    "    chained_bijectors = flow.bijector.bijector.bijectors\n",
    "    base_dist = flow.distribution\n",
    "    cols = len(chained_bijectors) +1\n",
    "    fig, ax = plt.subplots(1,cols,figsize=(4*cols,4))\n",
    "    \n",
    "    n=200\n",
    "\n",
    "    z_samples = np.linspace(-3,3,n).astype(np.float32)\n",
    "    log_probs = base_dist.log_prob(z_samples)\n",
    "\n",
    "    ax[0].plot(z_samples, np.exp(log_probs))\n",
    "\n",
    "    zz = z_samples[...,None]\n",
    "    ildj = 0.\n",
    "    for i,(a,b) in enumerate(zip(ax[1:],chained_bijectors)):\n",
    "        z = b.inverse(zz) # we need to use the inverse here since we are going from z->y!\n",
    "        ildj += b.forward_log_det_jacobian(z,1)\n",
    "        #print(z.shape, zz.shape, ildj.shape)\n",
    "        a.plot(z, np.exp(log_probs + ildj))\n",
    "        a.set_title(b.name.replace('_', ' '))\n",
    "        a.set_xlabel(f'$z_{i}$')\n",
    "        a.set_ylabel(f'$p(z_{i+1})$')\n",
    "        zz = z\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHCAbUNLJ6B5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_save_plf(model_names,x,y,figsize=(16,8),postfix=None,concat_df=None):\n",
    "    for a,m in zip(ax,model_names):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        df=plot_probabilistic_forecast(\n",
    "            models[m],\n",
    "            prob_models[m],\n",
    "            x=x)\n",
    "        plt.plot(y.flatten(),color='orange',label='load')\n",
    "        fig.suptitle(m.replace('_',' ').title())\n",
    "\n",
    "        if postfix is not None:\n",
    "            file_name=m + postfix + '.csv'\n",
    "            file_path=os.path.join(csv_path,file_name)\n",
    "            df['y']=y.flatten()\n",
    "            if concat_df is not None:\n",
    "                df = pd.concat([df,concat_df],axis='columns')\n",
    "            df.index.name = 't'\n",
    "            df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def flot_save_ridge_plots(model_names,x,y,postfix=None):\n",
    "    for m in model_names:\n",
    "        fig = stacked_dist_plot(models[m], prob_models[m], x, y)\n",
    "        if postfix is not None:\n",
    "            file_name=m + postfix + '.pgf'\n",
    "            file_path=os.path.join(tikz_path,file_name)\n",
    "            fig.savefig(file_path, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_observation(dist,y,ci=.90,tol=4):\n",
    "    yl=(1-ci)/2\n",
    "    yh=1-yl\n",
    "    pv = param_model(x)\n",
    "    dist = prob_model(pv)\n",
    "    ql = np.reshape([\n",
    "        m.bijector.forward(\n",
    "            m.distribution.quantile(yl)\n",
    "        ) for m in dist.distributions.model\n",
    "    ],(1,48))\n",
    "    qh = np.reshape([\n",
    "        m.bijector.forward(\n",
    "            m.distribution.quantile(yh)\n",
    "        ) for m in dist.distributions.model\n",
    "    ],(1,48))\n",
    "    score = ((y<ql) | (y>qh)).sum(axis=1)\n",
    "    return score <= tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalie_plot(prob_model, param_model,x,y,ci=.90,tol=4):\n",
    "    pv = param_model(x)\n",
    "    dist = prob_model(pv)\n",
    "    score = score_observation(dist, y, ci=ci, tol=tol)\n",
    "\n",
    "    c=['green' if ok else 'red' for ok in score]\n",
    "    a=[.5 if ok else .05 for ok in score]\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    for i in range(y.shape[0]):\n",
    "        plt.plot(\n",
    "            y[i],\n",
    "            c=c[i],\n",
    "            alpha=a[i]\n",
    "        )\n",
    "    plot_probabilistic_forecast(param_model, prob_model,x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_to_csv(x,y,file_name):\n",
    "    df=pd.DataFrame(x.reshape(-1,3).copy(),columns=columns)\n",
    "    df['y']=[np.nan]*(len(x)-48) + np.squeeze(y).tolist()\n",
    "    df['load'][-48:] = np.nan\n",
    "    df.plot(figsize=(16,8))\n",
    "    df.index.name='t'\n",
    "    df.to_csv(os.path.join(csv_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./configs/feed_forward_bernstein_flow.yaml',\n",
       " './configs/feed_forward_gmm.yaml',\n",
       " './configs/feed_forward_quantile_regression.yaml',\n",
       " './configs/feed_forward_normal_distribution.yaml']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_files=file_list(cfg_path,'yaml')\n",
    "cfg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Configuration(\n",
       "   compile_kwds={   'loss': <bernstein_paper.losses.multivariate_bernstein_flow_loss.MultivariateBernsteinFlowLoss object at 0x7fd504470f60>,\n",
       "     'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd504479278>},\n",
       "   data_loader_kwds={   'batch_size': 32,\n",
       "     'cycle_length': 10,\n",
       "     'data_path': '../paper_data/mini',\n",
       "     'history_columns': ['load'],\n",
       "     'history_size': 672,\n",
       "     'meta_columns': [   'dayofyear_sin',\n",
       "                         'dayofyear_cos',\n",
       "                         'weekday',\n",
       "                         'time_sin',\n",
       "                         'time_cos',\n",
       "                         'is_holiday'],\n",
       "     'prediction_columns': ['load'],\n",
       "     'prediction_size': 48,\n",
       "     'seed': 42,\n",
       "     'shift': 48,\n",
       "     'shuffle_buffer_size': 100,\n",
       "     'validation_split': 0.1},\n",
       "   data_preprocessor=None,\n",
       "   evaluate_kwds={},\n",
       "   fit_kwds={   'callbacks': [   <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd5044799b0>,\n",
       "                      <tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd6e57ae5c0>,\n",
       "                      <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd50440ce80>,\n",
       "                      <tensorflow.python.keras.callbacks.CSVLogger object at 0x7fd504411438>],\n",
       "     'epochs': 150,\n",
       "     'shuffle': True,\n",
       "     'validation_freq': 1},\n",
       "   model_checkpoints='./logs/feed_forward_bernstein_flow/mcp',\n",
       "   name='feed_forward_bernstein_flow',\n",
       "   seed=42\n",
       " ),\n",
       " Configuration(\n",
       "   compile_kwds={   'loss': <bernstein_paper.losses.mixed_density_loss.MixtedDensityLoss object at 0x7fd50439dda0>,\n",
       "     'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd5043a7da0>},\n",
       "   data_loader_kwds={   'batch_size': 32,\n",
       "     'cycle_length': 10,\n",
       "     'data_path': '../paper_data/mini',\n",
       "     'history_columns': ['load'],\n",
       "     'history_size': 672,\n",
       "     'meta_columns': [   'dayofyear_sin',\n",
       "                         'dayofyear_cos',\n",
       "                         'weekday',\n",
       "                         'time_sin',\n",
       "                         'time_cos',\n",
       "                         'is_holiday'],\n",
       "     'prediction_columns': ['load'],\n",
       "     'prediction_size': 48,\n",
       "     'seed': 42,\n",
       "     'shift': 48,\n",
       "     'shuffle_buffer_size': 100,\n",
       "     'validation_split': 0.1},\n",
       "   data_preprocessor=None,\n",
       "   evaluate_kwds={},\n",
       "   fit_kwds={   'callbacks': [   <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd5043ae6a0>,\n",
       "                      <tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd5043a7d30>,\n",
       "                      <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd5043bbb70>,\n",
       "                      <tensorflow.python.keras.callbacks.CSVLogger object at 0x7fd5043bb748>],\n",
       "     'epochs': 50,\n",
       "     'shuffle': True,\n",
       "     'validation_freq': 1},\n",
       "   model_checkpoints='./logs/feed_forward_gmm/mcp',\n",
       "   name='feed_forward_gmm',\n",
       "   seed=42\n",
       " ),\n",
       " Configuration(\n",
       "   compile_kwds={   'loss': <bernstein_paper.losses.pinball_loss.PinballLoss object at 0x7fd5042ccb00>,\n",
       "     'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd5042d9a20>},\n",
       "   data_loader_kwds={   'batch_size': 32,\n",
       "     'cycle_length': 10,\n",
       "     'data_path': '../paper_data/mini',\n",
       "     'history_columns': ['load'],\n",
       "     'history_size': 672,\n",
       "     'meta_columns': [   'dayofyear_sin',\n",
       "                         'dayofyear_cos',\n",
       "                         'weekday',\n",
       "                         'time_sin',\n",
       "                         'time_cos',\n",
       "                         'is_holiday'],\n",
       "     'prediction_columns': ['load'],\n",
       "     'prediction_size': 48,\n",
       "     'seed': 42,\n",
       "     'shift': 48,\n",
       "     'shuffle_buffer_size': 100,\n",
       "     'validation_split': 0.1},\n",
       "   data_preprocessor=None,\n",
       "   evaluate_kwds={},\n",
       "   fit_kwds={   'callbacks': [   <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd5042df240>,\n",
       "                      <tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd5042d97b8>,\n",
       "                      <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd5042ecc88>,\n",
       "                      <tensorflow.python.keras.callbacks.CSVLogger object at 0x7fd5042eccc0>],\n",
       "     'epochs': 150,\n",
       "     'shuffle': True,\n",
       "     'validation_freq': 1},\n",
       "   model_checkpoints='./logs/feed_forward_qunatile_regression/mcp',\n",
       "   name='feed_forward_qunatile_regression',\n",
       "   seed=42\n",
       " ),\n",
       " Configuration(\n",
       "   compile_kwds={   'loss': <bernstein_paper.losses.normal_distribution_loss.NormalDistributionLoss object at 0x7fd504282940>,\n",
       "     'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd5042885f8>},\n",
       "   data_loader_kwds={   'batch_size': 32,\n",
       "     'cycle_length': 10,\n",
       "     'data_path': '../paper_data/mini',\n",
       "     'history_columns': ['load'],\n",
       "     'history_size': 672,\n",
       "     'meta_columns': [   'dayofyear_sin',\n",
       "                         'dayofyear_cos',\n",
       "                         'weekday',\n",
       "                         'time_sin',\n",
       "                         'time_cos',\n",
       "                         'is_holiday'],\n",
       "     'prediction_columns': ['load'],\n",
       "     'prediction_size': 48,\n",
       "     'seed': 42,\n",
       "     'shift': 48,\n",
       "     'shuffle_buffer_size': 100,\n",
       "     'validation_split': 0.1},\n",
       "   data_preprocessor=None,\n",
       "   evaluate_kwds={},\n",
       "   fit_kwds={   'callbacks': [   <tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7fd504288da0>,\n",
       "                      <tensorflow.python.keras.callbacks.EarlyStopping object at 0x7fd504288588>,\n",
       "                      <tensorflow.python.keras.callbacks.ReduceLROnPlateau object at 0x7fd504219828>,\n",
       "                      <tensorflow.python.keras.callbacks.CSVLogger object at 0x7fd504219860>],\n",
       "     'epochs': 150,\n",
       "     'shuffle': True,\n",
       "     'validation_freq': 1},\n",
       "   model_checkpoints='./logs/feed_forward_normal_distribution/mcp',\n",
       "   name='feed_forward_normal_distribution',\n",
       "   seed=42\n",
       " )]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfgs = list(map(tfexp.configuration.Configuration.from_yaml, cfg_files))\n",
    "cfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models (Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: './logs/feed_forward_gmm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9d1b49ad8e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfgs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-9d1b49ad8e0d>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfgs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfexp/__init__.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: './logs/feed_forward_gmm'"
     ]
    }
   ],
   "source": [
    "models = {cfg.name: tfexp.build_model(cfg) for cfg in cfgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for m in models.values():\n",
    "    m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv = partial(pd.read_csv,index_col='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_paths = {cfg.name: os.path.split(cfg.model_checkpoints)[0] for cfg in cfgs}\n",
    "base_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dfs = {\n",
    "    k: pd.concat(\n",
    "        (\n",
    "            pd.read_csv(f)\n",
    "            for f in sorted(file_list(v,'csv'))\n",
    "        ),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    for k,v in base_paths.items() if len(file_list(v,'csv'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(hist_dfs),2, figsize=(16,8))\n",
    "if ax.size == 2:\n",
    "    ax = ax[np.newaxis]\n",
    "for i,(name,hist) in enumerate(hist_dfs.items()):\n",
    "    hist[['loss','val_loss']].plot(title=name, ax=ax[i,0])\n",
    "    hist[['lr']].plot(logy=True, title=name, ax=ax[i,1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({name: hist.val_loss.describe() for name,hist in hist_dfs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{name: hist.loss.min() for name,hist in hist_dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{name: hist.val_loss.min() for name,hist in hist_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernstein_paper.data.cer_data_loader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernstein_paper.data.loader import CSVDataLoader\n",
    "from bernstein_paper.data.pipeline import WindowedTimeSeriesPipeline\n",
    "from bernstein_paper.data.dataset import WindowedTimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9XYzCPXIPfY"
   },
   "outputs": [],
   "source": [
    "dl_kwds=cfg.data_loader_kwds\n",
    "dl_kwds.pop('validation_split')\n",
    "\n",
    "dl_kwds.update({'cycle_length': 1, 'shuffle_buffer_size': 0, 'batch_size': 7})\n",
    "data_path=dl_kwds.pop('data_path')\n",
    "\n",
    "dl_kwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformers = {}\n",
    "column_transformers['load'] = lambda x: tf.sqrt(x / 14.134)\n",
    "column_transformers['weekday'] = lambda x: tf.one_hot(\n",
    "    tf.cast(tf.squeeze(x), tf.uint8), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\n",
    "    os.path.join(data_path,'test.csv'),\n",
    "    parse_dates=['date_time'],\n",
    "    infer_datetime_format=True,\n",
    "    index_col=['date_time'],\n",
    "    dtype={'id': 'uint16',\n",
    "           'load': 'float32',\n",
    "           'is_holiday': 'uint8',\n",
    "           'weekday': 'uint8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ds=WindowedTimeSeriesDataSet(**dl_kwds, column_transformers=column_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=gen_ds(data)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_models = [\n",
    "    c for c in models.keys() if 'log' not in c and not c.endswith('gmm')\n",
    "]\n",
    "eval_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_prob_model(name):\n",
    "    if 'flow' in name:\n",
    "        return MultivariateBernsteinFlow\n",
    "    elif 'gmm' in name:\n",
    "        return MixedNormal()\n",
    "    elif 'qunatile_regression' in name:\n",
    "        return PinballLoss.constrain_quantiles\n",
    "    elif 'normal_distribution' in name:\n",
    "        #return NormalDistribution()\n",
    "        return lambda o: tfd.Normal(loc=o[:,:,0],scale=1e-3 + tf.math.softplus(0.05 * o[:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "prob_models={\n",
    "    m: match_prob_model(m)\n",
    "    for m in eval_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyO90MZeHnBN"
   },
   "source": [
    "## Plot patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EYY60RJoT7a"
   },
   "outputs": [],
   "source": [
    "x_hdim=48\n",
    "x_vdim=dl_kwds['history_size']//x_hdim\n",
    "y_hdim=48\n",
    "y_vdim=dl_kwds['prediction_size']//y_hdim\n",
    "\n",
    "N=2\n",
    "height_ratios=[len(dl_kwds['history_columns'])*(x_vdim+y_vdim),\n",
    "               len(dl_kwds['prediction_columns'])*y_vdim]\n",
    "fig_height=sum(height_ratios) + min(len(dl_kwds['history_columns']),1) + min(len(dl_kwds['prediction_columns']),1)\n",
    "fig_height/=5\n",
    "fig_width=N*(max(x_hdim, y_hdim) + min(N-1,1))/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pmEhhWtIcMF"
   },
   "outputs": [],
   "source": [
    "plot_patches_partial = partial(\n",
    "    plot_patches,\n",
    "    N=N,\n",
    "    x_hdim=x_hdim,\n",
    "    x_vdim=x_vdim,\n",
    "    y_hdim=y_hdim,\n",
    "    y_vdim=y_vdim,\n",
    "    historic_columns=dl_kwds['history_columns'],\n",
    "    horizon_columns=dl_kwds['meta_columns'],\n",
    "    prediction_columns=dl_kwds['prediction_columns'],\n",
    "    title_map={'x': 'Input Data',\n",
    "            'y': 'Prediction Target'},\n",
    "    y_label_map={\n",
    "     'x': {\n",
    "         'load': 'Load',\n",
    "         'tempC': 'Temperature',\n",
    "         'is_holiday': 'Is Holiday'},\n",
    "     'y': {\n",
    "         'load': 'Load'}},\n",
    "    fig_kw={'figsize': (fig_width, fig_height)},\n",
    "    heatmap_kw={\n",
    "     'x': {\n",
    "         'load': {'cmap': 'OrRd'},\n",
    "         'tempC': {'cmap': 'RdBu_r'},\n",
    "         'is_holiday': {'cmap': 'binary'}},\n",
    "     'y': {\n",
    "         'load': {'cmap': 'OrRd'}}},\n",
    "    gridspec_kw={'height_ratios': height_ratios,\n",
    "              'hspace': 2 / fig_height,\n",
    "              'wspace': 1 / fig_width},\n",
    "    xy_ch_connect=(\n",
    "         ('load', 0),\n",
    "         ('load', x_vdim + y_vdim - 1 - dl_kwds['shift'] // x_hdim))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig=plot_patches_partial(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.date.min()+pd.offsets.Week(2)#-pd.offsets.Minute(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week=data.loc[str(data.index.date.min()):str(data.index.date.min()+pd.offsets.Week(3)-pd.offsets.Minute(1))]\n",
    "f'firts week ranges from {first_week.index.min()+pd.offsets.Week(1)} to {first_week.index.max()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_ds = gen_ds(first_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = first_week_ds.as_numpy_iterator()\n",
    "\n",
    "(test_x1, test_x2),test_y = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plot_patch_ts(\n",
    "    x=test_x1,\n",
    "    y=test_y,\n",
    "    historic_columns=dl_kwds['history_columns'],\n",
    "    horizon_columns=[],#dl_kwds['horizon_columns'],\n",
    "    prediction_columns=dl_kwds['prediction_columns'],\n",
    "    fig_kw=dict(figsize=(16, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtZyOwtmJrmr"
   },
   "source": [
    "# Plot Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time=pd.date_range(first_week.index.min()+pd.offsets.Week(1),first_week.index.max()+pd.offsets.Minute(1),freq='30T')\n",
    "date_time=date_time.to_frame(index=False,name='date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_save_plf(eval_models,test_x[:7],test_y[:7],postfix='_fc_week',concat_df=date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x=(test_x1[:1],test_x2[:1])\n",
    "y=test_y[:1]\n",
    "for m in eval_models:\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    df=plot_probabilistic_forecast(\n",
    "        models[m],\n",
    "        prob_models[m],\n",
    "        x=x)\n",
    "    plt.plot(y.flatten(),color='orange',label='load')\n",
    "    fig.suptitle(m.replace('_',' ').title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m='feed_forward_bernstein_flow'\n",
    "model=models[m]\n",
    "pmodel=match_prob_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=pmodel(model((test_x1[:1],test_x2[:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernstein_flow.util.visualization import plot_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow=dist.distributions.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y_samples = np.linspace(0, 1, n, dtype=np.float32)[...,np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plot_flow(flow,y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows=pmodel(model((test_x1[:1],test_x2[:1]))).distributions.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = {\n",
    "    f\"gmm t={t}$\": m\n",
    "    for t,m in enumerate(flows)\n",
    "}\n",
    "\n",
    "yy = tf.cast(tf.linspace(y.min()-.1, y.max() + .1, int(200)), tf.float32)\n",
    "yy = yy[...,tf.newaxis]\n",
    "\n",
    "fig,axs=plt.subplots(len(yhats),figsize=(16,20*2))\n",
    "for (i,(name,yhat)),ax in zip(enumerate(yhats.items()),list(axs)):\n",
    "    plot_flow(yhat,yy,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m='feed_forward_qunatile_regression'\n",
    "model=models[m]\n",
    "pmodel=match_prob_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=pmodel(model((test_x1[:1],test_x2[:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.plot(test_y[1])\n",
    "num_quantiles=out.shape[-1]\n",
    "for q in range(num_quantiles):\n",
    "    plt.plot(out[0,...,q], ':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day=5\n",
    "flot_save_ridge_plots(eval_models,test_x[day],test_y[day],'_ridge_plot_week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet Bernstein Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = ds.as_numpy_iterator()\n",
    "test_x,test_y = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_model=models['wavenet_bernstein_flow']\n",
    "prob_model=prob_models['wavenet_bernstein_flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=test_x[0][None,...]\n",
    "y=np.squeeze(test_y)#np.squeeze(np.stack([test_y[0],test_y[1],test_y[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = prob_model(param_model(x))\n",
    "flow = dist.distributions.model[14]\n",
    "plot_chained_bijectors(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=np.linspace(0,1,100)\n",
    "y_probs=flow.prob(yy[...,tf.newaxis])\n",
    "bars=y_probs*1/(len(y_probs)-1)\n",
    "sum(bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = [sum(bars[:i]) for i in range(len(y_probs))]\n",
    "plt.plot(yy,cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalie Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = ds.as_numpy_iterator()\n",
    "test_x,test_y = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_observation(dist,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalie_plot(prob_model, param_model,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = score_observation(prob_model(param_model(x)),y)\n",
    "df=probabilistic_model_stats(prob_model, param_model(x))\n",
    "df['y']=y[0]\n",
    "file_name='anomalie_plot.csv'\n",
    "file_path=os.path.join(csv_path,file_name)\n",
    "\n",
    "y_ok=pd.DataFrame(np.squeeze(test_y[ok]).T).add_prefix('ok')\n",
    "y_nok=pd.DataFrame(np.squeeze(test_y[~ok]).T).add_prefix('nok')\n",
    "\n",
    "df=pd.concat([df,y_ok,y_nok],axis='columns')\n",
    "df.index.name='t'\n",
    "\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampleing from the learned distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=500\n",
    "samples=np.squeeze(dist.sample(n))\n",
    "samples.min(),samples.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[(samples.max(axis=1) < 2*y[0].max()) & (samples.min(axis=1) >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "fig=plt.plot(samples.T, color='gray', alpha=.1)\n",
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='samples.csv'\n",
    "file_path=os.path.join(csv_path,file_name)\n",
    "df=probabilistic_model_stats(prob_model, param_model(x))\n",
    "df['y']=y[0]\n",
    "\n",
    "ok = score_observation(prob_model(param_model(x)),samples)\n",
    "\n",
    "df_s=pd.DataFrame(samples[ok].T).add_prefix('s')\n",
    "df=pd.concat([df,df_s],axis='columns')\n",
    "df.index.name='t'\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalie_plot(prob_model, param_model,x,samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Christmas Test Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_days = data['2010-12-25':'2010-12-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_ids=(2.5*data.groupby('id').load.resample('D').sum().groupby('id').mean() < christmas_days.groupby('id').load.resample('D').sum().groupby('id').mean())\n",
    "interesting_ids=interesting_ids[interesting_ids].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interesting_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_patch = data['2010-12-16':'2010-12-29']\n",
    "christmas_patch = christmas_patch[christmas_patch.id.isin(interesting_ids)]\n",
    "f'christmas week ranges from {christmas_patch.index.min()+pd.offsets.Week(1)} to {christmas_patch.index.max()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(christmas_patch.index.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=christmas_patch.pivot_table(index=christmas_patch.index.date,columns='id',values='load',aggfunc='sum')\n",
    "days_per_id=pt.count()\n",
    "incomplete_ids=days_per_id[days_per_id!=len(np.unique(christmas_patch.index.date))].index.tolist()\n",
    "incomplete_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_patch =  christmas_patch[~christmas_patch.id.isin(incomplete_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_patch[christmas_patch.id.isin(interesting_ids)].pivot_table(columns='id',values='load',index='date_time').plot(figsize=(16,8),subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_ds = gen_ds(\n",
    "    christmas_patch[christmas_patch.id==3139],\n",
    "    dl_kwds,\n",
    "    cycle_length=1,\n",
    "    batch_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = christmas_ds.as_numpy_iterator()\n",
    "test_x,test_y = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plot_patch_ts(\n",
    "    x=test_x,\n",
    "    y=test_y,\n",
    "    historic_columns=dl_kwds['historic_columns'],\n",
    "    horizon_columns=dl_kwds['horizon_columns'],\n",
    "    prediction_columns=dl_kwds['prediction_columns'],\n",
    "    fig_kw=dict(figsize=(16, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time=pd.date_range(christmas_patch.index.min()+pd.offsets.Week(1),christmas_patch.index.max()+pd.offsets.Minute(1),freq='30T')\n",
    "date_time=date_time.to_frame(index=False,name='date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day=2\n",
    "patch_to_csv(test_x[day],test_y[day], 'example_patch_christmas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtZyOwtmJrmr"
   },
   "source": [
    "## Plot Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_save_plf(eval_models,test_x,test_y,postfix='_fc_christmas',concat_df=date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flot_save_ridge_plots(eval_models,test_x[day],test_y[day],'_ridge_plot_christmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on whole test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls   = {m: models[m].evaluate(ds) for m in eval_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_df = pd.DataFrame(data=nlls.values(), index=[k.replace('_',' ').title() for k in nlls.keys()], columns=['NLL'])\n",
    "\n",
    "nll_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Christmas days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls_christ   = {m: models[m].evaluate(christmas_ds) for m in eval_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls_christ_df = pd.DataFrame(data=nlls_christ.values(), index=[k.replace('_',' ').title() for k in nlls_christ.keys()], columns=['NLL'])\n",
    "\n",
    "nlls_christ_df.sort_index()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "wavenet_cnn_fc.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
